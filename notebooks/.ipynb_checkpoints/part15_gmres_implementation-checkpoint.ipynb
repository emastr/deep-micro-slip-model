{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "870ef026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import typing\n",
    "from torch.nn.functional import normalize\n",
    "from torch import tensordot as dot\n",
    "from torch.linalg import vector_norm as norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd6648f",
   "metadata": {},
   "source": [
    "We first make an implementation of Gram-Schmidt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "90c31506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_schmidt(x: Tensor, B: Tensor):\n",
    "    \"\"\"\n",
    "    Tensor x: shape (1, M) <- Representing NxC vectors in R^M\n",
    "    Tensor B: shape (R, M) <- Representing NxC vectors in R^M\n",
    "    \n",
    "    h0 = B' * x (pointwise dot products) -> (C, R)\n",
    "    \"\"\"\n",
    "    \n",
    "    h0 = dot(x, B, dims=[[1], [1]])             # dim (1, R)\n",
    "    x0 = x - dot(h0, B, dims=[[1], [0]])        # dim (1, M) - ((1, R) x (R, M)) = # dim (1, M)\n",
    "\n",
    "    h1 = dot(x0, B, dims=[[1], [1]])            # dim (1, R)\n",
    "    x1 = x0 - dot(h1, B, dims=[[1], [0]])       # dim (1, M) - ((1, R) x (R, M)) = # dim (1, M) \n",
    "    \n",
    "    h = h0 + h1                                             # dim (1, R)\n",
    "    b = norm(x1, dim=1, keepdim=True)   # dim (1, 1)        \n",
    "    \n",
    "    x1 = x1 / b\n",
    "    \n",
    "    return x1, h, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "cd7d90fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arnoldi(op, b, steps, callback=None):\n",
    "    \"\"\"\n",
    "    nn.Module op: takes (C, M) input, outputs of shape (C, M).\n",
    "    Tensor b: (1, M)\n",
    "    \"\"\"\n",
    "    assert steps < b.shape[1], f\"Hey, steps must be at most {b.shape[1]}\"\n",
    "    \n",
    "    q = b / norm(b, dim=1)  # dim (C, M)\n",
    "    Q = torch.zeros(steps+1, b.shape[1]) # dim (C, M)\n",
    "    Q[0, :] = torch.ones(q.shape[1])\n",
    "    Q = Q * q\n",
    "    for m in range(steps):\n",
    "        # Next element in krylov space\n",
    "        x = op(q)\n",
    "        \n",
    "        # Orthogonalise against Q and update\n",
    "        (q, h, beta) = gram_schmidt(x, Q[:(m+1), :])\n",
    "        \n",
    "        # Create H\n",
    "        # Hij = dot(q_i, (Aq)_;j) = q_i' * Aq_j\n",
    "        # Behöver endast beräkna q_m' * Aq_j  och q_j * Aqm\n",
    "        AQ = op(Q[:(m+1), :]) # -> (C, M)\n",
    "        H = dot(Q[:(m+1), :], AQ, dims=[[1],[1]]) # (C, C) # unnecessary double counting but ok\n",
    "        \n",
    "        # Do callback\n",
    "        stop = False\n",
    "        if callback is not None:\n",
    "            stop = callback(Q[:(m+1),:], q, H, beta, m+1)\n",
    "        if stop:\n",
    "            break\n",
    "        mask = torch.zeros_like(Q)\n",
    "        mask[m+1, :] = torch.ones_like(q)\n",
    "        Q = Q + mask * q\n",
    "    return Q, q, H, beta\n",
    "\n",
    "\n",
    "\n",
    "def gmres(op, b, steps, callback=None, verbose=False):\n",
    "    \"\"\"\n",
    "    GMRES implementation in pytorch\n",
    "    \"\"\"\n",
    "    Q, q, H, beta = arnoldi(op, b, steps, callback=callback)\n",
    "    \n",
    "    normb = norm(b, dim=1)\n",
    "    m = steps\n",
    "    Q = Q[:m, :]\n",
    "    \n",
    "    em = torch.zeros(m, m)\n",
    "    em[m-1, m-1] = 1\n",
    "\n",
    "    e1 = torch.zeros(1, m)\n",
    "    e1[0, 0] = 1\n",
    "\n",
    "    HmTHm = dot(H, H, dims=[[0], [0]]) + em * beta**2\n",
    "    HmTbe = dot(normb*e1, H, dims=[[1], [0]])\n",
    "\n",
    "    z = torch.linalg.solve(HmTHm[None,:,:], HmTbe)\n",
    "    x = dot(z, Q, dims=[[1],[0]])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "fcc83f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5018e-07, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "A = 0.1*torch.rand(n,n) + torch.eye(n)\n",
    "x = torch.ones(n)[None,:]\n",
    "b.requires_grad = True\n",
    "\n",
    "op = lambda x: torch.tensordot(x, A, dims=[[1],[1]]) # (C, M) x (M, M) -> (C, M)\n",
    "y = gmres(op, b, steps=5, verbose=True)\n",
    "\n",
    "print(dot(y,A,dims=[[1],[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23baf5e",
   "metadata": {},
   "source": [
    "Let's try GMRES on our integral operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea41272",
   "metadata": {},
   "source": [
    "Below is an in-place version of gmres which is faster, but not differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd162d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gmres(op, b, steps, tol=0., callback=None, verbose=False):\n",
    "    \"\"\"\n",
    "    GMRES implementation in pytorch\n",
    "    \"\"\"\n",
    "    \n",
    "    normb = torch.linalg.vector_norm(b, dim=1)\n",
    "    x = torch.zeros_like(b)\n",
    "    \n",
    "    def gmres_callback(Q, q, H, beta, m):\n",
    "        em = torch.zeros(1, m)\n",
    "        em[0, m-1] = 1\n",
    "        \n",
    "        e1 = torch.zeros(1, m+1)\n",
    "        e1[0, 0] = 1\n",
    "        \n",
    "        Hm = torch.zeros(m+1, m)\n",
    "        Hm[:m, :m] = H\n",
    "        Hm[m,   :] = em * beta\n",
    "        \n",
    "        be1 = normb  * e1\n",
    "        \n",
    "        HmTHm = torch.tensordot(Hm, Hm , dims=[[0], [0]])\n",
    "        HmTbe = torch.tensordot(be1, Hm, dims=[[1], [0]])\n",
    "        \n",
    "        z = torch.linalg.solve(HmTHm[None,:,:], HmTbe)\n",
    "        x[:] = torch.tensordot(z, Q, dims=[[1],[0]])\n",
    "        \n",
    "        res = torch.linalg.vector_norm((torch.tensordot(z, Hm, dims=[[1], [1]]) - be1)[0])\n",
    "        stop = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"residual at step {m}: {res:.2e}\")\n",
    "        if res < tol:\n",
    "            stop = True\n",
    "        if callback is not None:\n",
    "            stop = stop or callback(Q, q, H, beta, m)\n",
    "        return stop\n",
    "    \n",
    "    arnoldi(op, b, steps, callback=gmres_callback)\n",
    "    return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Physics Informed Machine Learning Environment",
   "language": "python",
   "name": "pimlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
