{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes for training \n",
    "\n",
    "* Adjust learning rate: ``batch_size`` / ``learning_rate`` = const. See [this blog](https://medium.com/deep-learning-experiments/effect-of-batch-size-on-neural-net-training-c5ae8516e57). Might have to do with normalisation details.\n",
    "* H1 loss considerably better than L2 loss it seems.\n",
    "* Distance to center\n",
    "* Optimal Kernel size? 60 better than 40 better than 20\n",
    "* More data = good. Implement data loader for GPU? \n",
    "* Decide on features. \n",
    "  * Can have coordinates + derivatives + input, or natural coordinates + curvature.\n",
    "  * Natural coordinates + curvature results in worse performance\n",
    "  * Reparameterisation to uniform (not GL-points)\n",
    "  * [DMD](https://arxiv.org/pdf/1409.6358.pdf)\n",
    "  * <span style=\"color:red\"> IDEA: Generalize our results to non-linear projection mappings.</span> Let $u$ be micro data and $U$ macro data.\n",
    "    *   Instead of $\\langle a(u), U\\rangle$, generalize to $\\sigma(\\langle\\pmb{a}_2(u), \\sigma(\\langle \\pmb{a}_1(u), U\\rangle))\\rangle)$ or even\n",
    "        $$\n",
    "        z_{k+1} = z_k + \\sigma(W_k(u)z_k + b_k(u))\n",
    "        $$ \n",
    "    * Geo-FNO fits in this framework. Rescale FNO, but canonical arclength parameterisation means det() = 1.\n",
    "* Uncertainty quantification\n",
    "  * Sampling from posterior\n",
    "    * HMC - Hamiltonian monte carlo\n",
    "    * MH - Metropolis-hastings\n",
    "    * No U-turn\n",
    "    * Langevin dynamics\n",
    "    * Robinson-Monro\n",
    "    * MFVI - Mean-Field Variational inference\n",
    "    * Monte Carlo Dropout, also during inference - obtain mean + std\n",
    "    * Laplace approximation\n",
    "    * Deep Ensebles / Snapshot encebles (cyclic learning rate)\n",
    "    * Stochastic weight averaging gaussian (SWAG)\n",
    "  * Bayesian Pinn - bayesian neural networks\n",
    "    * weights + biases follow a distribution\n",
    "    * Gappy data: Gaussian process regression\n",
    "    * Better to use Gaussian Process Regression for UQ\n",
    "    * ELBO (evidence lower bound) = KL divergence measurement thing\n",
    "    * [Multi-fidelity](https://www.sciencedirect.com/science/article/pii/S0021999119307260?ref=pdf_download&fr=RR-2&rr=7dc5cafa9f9d09a3) bayesian nn: High Fidelity + Low Fidelity data, train low fidelity NN \n",
    "  * META learning\n",
    "    * \"Learning to learn\"\n",
    "    * Model Agnostic Meta-Learning (MAML)\n",
    "  * Functional Prior\n",
    "    * Gaussian prior + historical observations -> generate functional priors\n",
    "    * Functional prior + new observations -> posterior\n",
    "    * Latent space | neural space | functional space\n",
    "  * Uncertainty quantification in scientific machine learning (Karniadakis)\n",
    "    * Neural UQ library / Hamiltonian Mnte Carlo conceptual intro / \n",
    "    * Epistemic uncertainty vs aliatoric uncertainty (account for noise in data?)\n",
    "* Architecture\n",
    "  * PINN \n",
    "    * Main drawback: Retrain for each new problem.\n",
    "    * Compute derivatives in forward mode.\n",
    "    * Compute gradients of loss function in reverse mode.\n",
    "    * Adversarial sampling / adversarial weighting\n",
    "    * Self adaptive loss weights\n",
    "    * Dynamic weights for PINN (weights vary over domain)\n",
    "    * Hard constraints: compact fcn, periodic, rotation of scalar field\n",
    "    * hp-VPINN (global NN, local weighting), VPINN, VarNet, D3M, conservative VPINN\n",
    "    * XPINN, cPINN - domain decomposition\n",
    "    * FPINN - fractional derivatives\n",
    "    * SPINN - Pi-GAN, physics informed GAN\n",
    "    * Separable PINN: rensor products\n",
    "    * Sobolev training for PINN\n",
    "    * ENG [Marius](https://arxiv.org/pdf/2302.13163v1.pdf)\n",
    "      * **Observations**\n",
    "        * Gramiam better with more points (smoother loss)\n",
    "        * Works best for smooth landscape (struggles with sin(3x))\n",
    "        * Inversion scales like O(n^3)\n",
    "        * Gramiam is poorly conditioned (k > 10^10)\n",
    "        * Sparse (only a few entries have high curvature)\n",
    "        * Converges such that $\\nabla L_\\Omega(\\theta) = \\nabla L_{\\partial\\Omega}(\\theta)$. If $L_\\Omega << L_{\\partial\\Omega}$, we're stuck here\n",
    "      * **Methods**\n",
    "        * Gauss-Seidel? Conjugate gradient? Coordinate Newton descent?\n",
    "        * [Stochastic Conjugate Gradient](https://arxiv.org/pdf/1710.09979.pdf)\n",
    "        * Jax uses a [LAPACK implementation of least squares](https://netlib.org/lapack/explore-html/d7/d3b/group__double_g_esolve_ga94bd4a63a6dacf523e25ff617719f752.html), based on [Householder](https://en.wikipedia.org/wiki/Singular_value_decomposition) transforms to find the singular values. \n",
    "        * [Competetive gradient](https://arxiv.org/pdf/1905.12103.pdf) [Competetive pinn](https://arxiv.org/pdf/2204.11144.pdf)\n",
    "        * Energy natural gradient is actually [Gauss-newton](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm), with an appropriate choice for the residuals.\n",
    "        * Add diagonal to matrix: Regularised gauss-newton [here 1](https://arxiv.org/pdf/2112.02089.pdf) [here 2, called damping](https://arxiv.org/pdf/2010.00879.pdf) can also be viewed as [Tikhonov regularisation](https://arxiv.org/pdf/1412.1193.pdf), or [Levenberg-Marquart](https://github.com/google/jaxopt/blob/main/jaxopt/_src/levenberg_marquardt.py). [Gauss-Newton ](https://arxiv.org/pdf/2306.08727.pdf) [Lev-Marq 2](https://arxiv.org/ftp/arxiv/papers/2111/2111.06060.pdf)\n",
    "        * The [Does Optimisation matter?](https://arxiv.org/pdf/2002.12642.pdf)\n",
    "        * Raj: [optax](https://optax.readthedocs.io/en/latest/api.html?highlight=adam#adam)\n",
    "          *  GPU: Memory should be saturated. CPU not as important. \n",
    "          * Use cpython profiler to check flop counts\n",
    "    * FNO Karniadakis felt Honest\n",
    "      * PINO  - Physics-informed neural operator.\n",
    "  * Unrolled gradient\n",
    "    * Feature maps that evaluate integral operator\n",
    "    * Operations are too costly\n",
    "  * [FNO](https://github.com/neuraloperator/neuraloperator/)\n",
    "    * [Comparison to DeepOnet by Karniadakis](https://arxiv.org/pdf/2111.05512.pdf)\n",
    "    * Best performance so far. (40 modes with a lot of data)\n",
    "    * Best result on low variability data with anchored parameterisation\n",
    "    * **Relative loss to spread error - big difference**\n",
    "    * **H1 Penalty - better generalisation**\n",
    "    * Invariance/equivariance. [Clifford layers](https://arxiv.org/pdf/2209.04934.pdf)\n",
    "      * fno, curvature + natural coordinates + relu + no bias [invariant](https://proceedings.neurips.cc/paper_files/paper/2022/file/5aea56eefab60e06f35016478e21aae6-Paper-Conference.pdf) [invariant 2](https://arxiv.org/pdf/2006.15646.pdf)\n",
    "      * Gives equinvariance wrt rescale, rotate, and circular shift\n",
    "      * **INVARIANT!** Use only normalised curvature + natural coordinates, can use bias + grelu. \n",
    "    * Decimate and train on low resolution data (multigrid). \n",
    "      * Same loss, likely because the initial frequencies are not initially tuned.\n",
    "      * V-cycle?\n",
    "  * [Geo-FNO](https://arxiv.org/pdf/2207.05209.pdf) (geometry-aware FNO) \n",
    "  * FNOproj\n",
    "    * Force linearity of map: map directly to eigen basis.\n",
    "    * Performs well, not as well as FNO on low res data\n",
    "  * U-FNO\n",
    "    * Performs well with a lot of parameters\n",
    "    * Too costly in comparison to FNO\n",
    "  * FNO-conv?\n",
    "    * Parallel: Spectral Ewald\n",
    "    * Close range interactions with kernel\n",
    "  * Ideas\n",
    "    * \"Spherical Convolutions\"\n",
    "    * \"Normalizing inputs\"\n",
    "    * [Map to source field](https://academic.oup.com/gji/article/205/1/575/2594866?login=true)\n",
    "    * \"Equivalent maps\"\n",
    "    * Don't plot input vs output\n",
    "    * [Linearly constrained](https://arxiv.org/pdf/2002.01600.pdf)\n",
    "  * [Wavelet Neural Operator](https://github.com/TapasTripura/Wavelet-Neural-Operator-for-pdes/blob/main/wno_2d_AC.py), Article [here](https://arxiv.org/pdf/2205.02191.pdf)\n",
    "  * [Deep M&M net](https://arxiv.org/pdf/2009.12935.pdf)\n",
    "  * FNO-former\n",
    "  * DeepOnet\n",
    "    * Branch-Net, trunk net. Recurrent for time.\n",
    "    * **Better Feature map (Legendre, Fourier) -> better fit**, lower generalisation error.\n",
    "      * Karhunen-Loeve (KL) expansion, expand process into \"eigen modes\"\n",
    "      * Karhunen Loeve transform, Hotelling transform\n",
    "    * Good at one-shot, important to give right data distribution\n",
    "    * DeepOnet Transfer learning / doman adaptation\n",
    "      * Multiple domains -> change the basis functions of branch net\n",
    "  * [ViTO](https://arxiv.org/pdf/2303.08891.pdf)\n",
    "    * U-Net architecture with vision transformer at the bottleneck\n",
    "    * Performs on par with FNO on other data.\n",
    "  * [DiTTO](https://github.com/lucidrains/DALLE-pytorch/discussions/375)\n",
    "    * Diffusion-Type Transformer Operator"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
